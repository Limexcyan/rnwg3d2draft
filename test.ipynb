{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ðŸ“˜ Title: Advanced Neural Rendering and Vision Techniques for Car Damage Detection\n",
    "\n",
    "# ## 1. Introduction\n",
    "\n",
    "\"\"\"\n",
    "This notebook explores a modern pipeline for detecting vehicle damage using the intersection of deep learning, semantic segmentation,\n",
    "vision transformers, and neural rendering (NeRFs and Gaussian Splatting).\n",
    "\n",
    "The goal is to not only achieve accurate damage detection, but also demonstrate:\n",
    "- 3D scene reconstruction using NeRF\n",
    "- Transfer learning with ViTs\n",
    "- Multimodal neural processing\n",
    "- Visualization of results in 3D and 2D\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch torchvision timm transformers matplotlib opencv-python imageio[ffmpeg] -q"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74a7c78e2c258259"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of columns must be a positive integer, not 0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     13\u001B[39m image_paths = \u001B[38;5;28msorted\u001B[39m(glob(\u001B[33m\"\u001B[39m\u001B[33msample_data/car_views/*.jpg\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m     14\u001B[39m images = [load_image(p) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m image_paths]\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m fig, axs = \u001B[43mplt\u001B[49m\u001B[43m.\u001B[49m\u001B[43msubplots\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfigsize\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, img \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(images):\n\u001B[32m     18\u001B[39m     axs[i].imshow(img)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\pythonProject\\venv\\Lib\\site-packages\\matplotlib\\pyplot.py:1770\u001B[39m, in \u001B[36msubplots\u001B[39m\u001B[34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001B[39m\n\u001B[32m   1625\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1626\u001B[39m \u001B[33;03mCreate a figure and a set of subplots.\u001B[39;00m\n\u001B[32m   1627\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   1767\u001B[39m \n\u001B[32m   1768\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1769\u001B[39m fig = figure(**fig_kw)\n\u001B[32m-> \u001B[39m\u001B[32m1770\u001B[39m axs = \u001B[43mfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43msubplots\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mncols\u001B[49m\u001B[43m=\u001B[49m\u001B[43mncols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msharex\u001B[49m\u001B[43m=\u001B[49m\u001B[43msharex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msharey\u001B[49m\u001B[43m=\u001B[49m\u001B[43msharey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1771\u001B[39m \u001B[43m                   \u001B[49m\u001B[43msqueeze\u001B[49m\u001B[43m=\u001B[49m\u001B[43msqueeze\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubplot_kw\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubplot_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1772\u001B[39m \u001B[43m                   \u001B[49m\u001B[43mgridspec_kw\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgridspec_kw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight_ratios\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheight_ratios\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1773\u001B[39m \u001B[43m                   \u001B[49m\u001B[43mwidth_ratios\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwidth_ratios\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m fig, axs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\pythonProject\\venv\\Lib\\site-packages\\matplotlib\\figure.py:918\u001B[39m, in \u001B[36mFigureBase.subplots\u001B[39m\u001B[34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001B[39m\n\u001B[32m    914\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[33mwidth_ratios\u001B[39m\u001B[33m'\u001B[39m\u001B[33m must not be defined both as \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    915\u001B[39m                          \u001B[33m\"\u001B[39m\u001B[33mparameter and as key in \u001B[39m\u001B[33m'\u001B[39m\u001B[33mgridspec_kw\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    916\u001B[39m     gridspec_kw[\u001B[33m'\u001B[39m\u001B[33mwidth_ratios\u001B[39m\u001B[33m'\u001B[39m] = width_ratios\n\u001B[32m--> \u001B[39m\u001B[32m918\u001B[39m gs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43madd_gridspec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mncols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfigure\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgridspec_kw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    919\u001B[39m axs = gs.subplots(sharex=sharex, sharey=sharey, squeeze=squeeze,\n\u001B[32m    920\u001B[39m                   subplot_kw=subplot_kw)\n\u001B[32m    921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m axs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\pythonProject\\venv\\Lib\\site-packages\\matplotlib\\figure.py:1600\u001B[39m, in \u001B[36mFigureBase.add_gridspec\u001B[39m\u001B[34m(self, nrows, ncols, **kwargs)\u001B[39m\n\u001B[32m   1557\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1558\u001B[39m \u001B[33;03mLow-level API for creating a `.GridSpec` that has this figure as a parent.\u001B[39;00m\n\u001B[32m   1559\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   1596\u001B[39m \n\u001B[32m   1597\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1599\u001B[39m _ = kwargs.pop(\u001B[33m'\u001B[39m\u001B[33mfigure\u001B[39m\u001B[33m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# pop in case user has added this...\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1600\u001B[39m gs = \u001B[43mGridSpec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mncols\u001B[49m\u001B[43m=\u001B[49m\u001B[43mncols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfigure\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1601\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m gs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\pythonProject\\venv\\Lib\\site-packages\\matplotlib\\gridspec.py:363\u001B[39m, in \u001B[36mGridSpec.__init__\u001B[39m\u001B[34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001B[39m\n\u001B[32m    360\u001B[39m \u001B[38;5;28mself\u001B[39m.hspace = hspace\n\u001B[32m    361\u001B[39m \u001B[38;5;28mself\u001B[39m.figure = figure\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mncols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    364\u001B[39m \u001B[43m                 \u001B[49m\u001B[43mwidth_ratios\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwidth_ratios\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m                 \u001B[49m\u001B[43mheight_ratios\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheight_ratios\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\pythonProject\\venv\\Lib\\site-packages\\matplotlib\\gridspec.py:51\u001B[39m, in \u001B[36mGridSpecBase.__init__\u001B[39m\u001B[34m(self, nrows, ncols, height_ratios, width_ratios)\u001B[39m\n\u001B[32m     48\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     49\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNumber of rows must be a positive integer, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnrows\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ncols, Integral) \u001B[38;5;129;01mor\u001B[39;00m ncols <= \u001B[32m0\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     52\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNumber of columns must be a positive integer, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mncols\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     53\u001B[39m \u001B[38;5;28mself\u001B[39m._nrows, \u001B[38;5;28mself\u001B[39m._ncols = nrows, ncols\n\u001B[32m     54\u001B[39m \u001B[38;5;28mself\u001B[39m.set_height_ratios(height_ratios)\n",
      "\u001B[31mValueError\u001B[39m: Number of columns must be a positive integer, not 0"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 3. Load and Display Images\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image_paths = sorted(glob(\"sample_data/car_views/*.jpg\"))\n",
    "images = [load_image(p) for p in image_paths]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "for i, img in enumerate(images):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"Multiple Views of Car\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-06T17:52:55.907297100Z",
     "start_time": "2025-05-06T17:52:54.520013400Z"
    }
   },
   "id": "915cf8ebba4614fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## 4. Preprocessing for Segmentation and Damage Detection\n",
    "\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "import torch\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
    "seg_model = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n",
    "seg_model.eval()\n",
    "\n",
    "def segment_car(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = seg_model(**inputs).logits\n",
    "    return logits.argmax(dim=1)[0].cpu().numpy()\n",
    "\n",
    "seg_masks = [segment_car(img) for img in images]\n",
    "\n",
    "# Visualize segmentation\n",
    "fig, axs = plt.subplots(1, len(seg_masks), figsize=(15, 5))\n",
    "for i, mask in enumerate(seg_masks):\n",
    "    axs[i].imshow(mask)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"Segmentation Masks\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7816a5ca4d63c5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## 5. ViT-based Damage Classifier (Fine-tuned)\n",
    "\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "\n",
    "# Assume fine-tuned model checkpoint is available\n",
    "class DamageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.vit.head = nn.Linear(self.vit.head.in_features, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "classifier = DamageClassifier()\n",
    "classifier.load_state_dict(torch.load(\"vit_damage_classifier.pth\"))\n",
    "classifier.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "for img in images:\n",
    "    x = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prob = torch.softmax(classifier(x), dim=1)[0][1].item()\n",
    "    print(f\"Damage Probability: {prob:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "544ec0ebfc19099a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# ---\n",
    "# ## 6. Neural Radiance Fields (NeRF) Pipeline (Optional)\n",
    "\n",
    "\"\"\"\n",
    "Instructions:\n",
    "- Store multiple car views in `instant-ngp/data/car_scene`\n",
    "- Use Instant-NGP's GUI to train and render the car scene\n",
    "- Export depth maps and mesh if needed\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d097ff98779b51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ---\n",
    "# ## 7. Post-Processing: Damage Overlay on Image\n",
    "\n",
    "highlighted_imgs = []\n",
    "for img, mask in zip(images, seg_masks):\n",
    "    damage_overlay = np.zeros_like(img)\n",
    "    damage_overlay[mask == 2] = [255, 0, 0]\n",
    "    combined = cv2.addWeighted(img, 0.7, damage_overlay, 0.3, 0)\n",
    "    highlighted_imgs.append(combined)\n",
    "\n",
    "fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "for i, img in enumerate(highlighted_imgs):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.suptitle(\"Detected Damage (Red Highlight)\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7b02e08b91c3bba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
